"0","# -----------------------------------"
"0","# get vertex index for each network "
"0","# -----------------------------------"
"0","# rm(list=ls())"
"0","# "
"0","# in_path <- ""/Users/peetal/Documents/Honor_project/GordonParcelTemplate/Parcels/"""
"0","# out.path <- ""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/"";"
"0","# g.key <- read.csv(""/Users/peetal/Documents/Honor_project/GordonParcelTemplate/Parcels.csv"", stringsAsFactors=FALSE); # which parcels in each community; "
"0","# setwd(""/Users/peetal/Documents/Honor_project/workbench/bin_macosx64/"")"
"0","# "
"0","# # make a text version of the Gordon atlas - parcel assignment for each vertex"
"0","# dt.fname <- paste0(out.path, ""Gordon_Parcels_LR.dtseries.txt"");"
"0","# if (!file.exists(dt.fname)) { "
"0","#   in.fname <- paste0(in_path, ""Parcels_LR.dtseries.nii"");     # part of the Gordon atlas download"
"0","#   if (!file.exists(in.fname)) { stop(paste(""missing:"", in.fname)); }"
"0","#   system(paste0(""./wb_command -cifti-convert -to-text "", in.fname, "" "", dt.fname));   # call the  wb_command function"
"0","# }"
"0","# in.dt <- read.table(dt.fname)[,1];   # just one column in the file"
"0","# "
"0","# # each community has a different number of component parcels, and vertices in each parcel."
"0","# # make a single file for each community listing *all* of its vertices."
"0","# networks <- c(""RetrosplenialTemporal"",""Auditory"",""SMmouth"",""SMhand"",""Visual"",""None"",""Default"","
"0","#               ""CinguloParietal"",""VentralAttn"",""CinguloOperc"",""Salience"",""DorsalAttn"",""FrontoParietal"")"
"0","# for (do.comm in networks) {   "
"0","#   for (do.hem in c(""L"", ""R"")) {   # do.comm <- ""Visual""; do.hem <- ""L"";"
"0","#     need.parcels <- g.key$ParcelID[which(g.key$Community == do.comm & g.key$Hem == do.hem)];  # parcels for this community & hemisphere"
"0","#     "
"0","#     # vertices (cifti rows) for each of these parcels"
"0","#     fout <- file(paste0(out.path, do.comm, ""_"", do.hem, ""_ciftirows.txt""), 'wt');   # start a blank text file for writing"
"0","#     for (pid in 1:length(need.parcels)) {   # pid <- 1;"
"0","#       inds <- which(in.dt == need.parcels[pid]);   # rows of the cifti for parcel need.parcel[pid] only"
"0","#       for (i in 1:length(inds)) { cat(inds[i], file=fout, sep=""\n""); }    # write out a line with each row number"
"0","#     }"
"0","#     close(fout); unlink(fout);    # let go of the file"
"0","#   }"
"0","# }"
"0","# "
"0","# # -------------------------------------------------------------------------"
"0","# # get vertex level neural data (COPE11) for each subject, for each network "
"0","# # -------------------------------------------------------------------------"
"0","# "
"0","# # important parameters"
"0","# in_path <- ""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/"";   # _ciftirows.txt files, cope and id keys, etc."
"0","# out_path <- ""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/"";   # files written out to here                       "
"0","# comm_ids <- networks; "
"0","# hem_ids <- c(""L"", ""R"");   "
"0","# "
"0","# "
"0","# # subject IDs"
"0","# ids <- import(here::here('./data/gordon333_cope11_rm_outlier.csv')) %>%"
"0","#   select(Subject) %>% unname() %>% unlist()"
"0","# missing_id <- 0"
"0","# "
"0","# "
"0","# for (cid in 1:length(comm_ids)) { #for each network #cid = 1"
"0","#   if (!dir.exists(paste0(in_path, ""Gordon/"",comm_ids[cid]))){ # if sub-folders for the network does not exist "
"0","#     dir.create(paste0(in_path, ""Gordon/"",comm_ids[cid]))"
"0","#   }"
"0","#   for (sub in 1:length(ids)) { #for each subject #sub = 2"
"0","#     "
"0","#     vertex_fname <- paste0(out_path, ""cope11_vertex_dat/"", ids[sub], ""_s1200_WMcope11s.txt"")"
"0","#     if (!file.exists(vertex_fname)) { missing_id <- missing_id + 1; next; }"
"0","#     "
"0","#     sub_L_R <- c()"
"0","#     for (hid in 1:length(hem_ids)) {   # for each hemisphere hid = 1;"
"0","#       # get the rows of the text-ized cifti for this community and hemisphere"
"0","#       fname <- paste0(in_path, ""Gordon/"", comm_ids[cid], ""_"", hem_ids[hid], ""_ciftirows.txt"");  "
"0","#       if (!file.exists(fname)) { stop(paste(""missing:"", fname)); }"
"0","#       need_rows <- read.table(fname)[,1];   # read in the file, just keeping the first column"
"0","#       "
"0","#       # get COPE data:"
"0","#       vertex <- read.table(vertex_fname)[,1]"
"0","#       sub_L_R <- append(sub_L_R, vertex[need_rows])"
"0","#     }"
"0","#     "
"0","#     # write out data"
"0","#     sub_out_tbl <- array(NA, c(1, length(sub_L_R)));   # blank table to hold the values we need to keep"
"0","#     rownames(sub_out_tbl) <- ""cope11"";"
"0","#     colnames(sub_out_tbl) <- paste0(""v"", 1:length(sub_L_R));"
"0","#     sub_out_tbl[1,] <- sub_L_R"
"0","#     out_fname <- paste0(out_path, ""Gordon/"", comm_ids[cid], ""/"", ids[sub], ""_"", comm_ids[cid], ""_cope11s.txt"")"
"0","#     write.table(sub_out_tbl, out_fname)"
"0","#   }"
"0","# }"
"0","# "
"0","# # --------------------"
"0","# # compute effect size"
"0","# # --------------------"
"0","# "
"0","# # function that computes vertex level effect size for a given network"
"0","# vertex_level_es <- function(dir) { # dir <- ""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/FrontoParietal/"""
"0","#   "
"0","#   data_from_dir <- Sys.glob(paste0(dir, '*.txt')) # all vertex within this network"
"0","#   nVertex <- ncol(read.table(data_from_dir[1])) # number of vertices in this network"
"0","#   nSub <- length(data_from_dir) # number of subjects "
"0","#   network_vertex_data <- data.frame(array(NA, c(nSub, nVertex + 1))) # create an empty dataframe"
"0","#   "
"0","#   for (i in 1:length(data_from_dir)){ # i = 1"
"0","#   "
"0","#     sub_id <- unlist(str_split(unlist(str_split(data_from_dir[i], ""/""))[10],""_""))[1] # subject id"
"0","#     network_vertex_data[i,1] <- sub_id # add subject id to the dataframe"
"0","#   "
"0","#     sub_data <- read.table(data_from_dir[i]) # extract in neural data "
"0","#     network_vertex_data[i,1:nVertex + 1] <- sub_data[1,] %>% unname() %>% unlist() # extract in neural data "
"0","#   }                                 "
"0","#   "
"0","#   # compute load effect size for all the vertices"
"0","#   cohenD <- network_vertex_data %>%"
"0","#     select(-X1) %>%"
"0","#     map_dbl(~ (mean(.)/sd(.))) # effect size for each parcel mean of the effect over sd of the data (Poldrack et al., 2017) "
"0","#   "
"0","#   return(cohenD)"
"0","# }"
"0","# "
"0","# "
"0","# RetrosplenialTemporal_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/RetrosplenialTemporal/"")"
"0","# Auditory_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Auditory/"")"
"0","# SMmouth_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/SMmouth/"")"
"0","# SMhand_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/SMhand/"")"
"0","# Visual_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Visual/"")"
"0","# None_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/None/"")"
"0","# Default_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Default/"")"
"0","# CinguloParietal_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/CinguloParietal/"")"
"0","# VentralAttn_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/VentralAttn/"")"
"0","# CinguloOperc_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/CinguloOperc/"")"
"0","# Salience_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Salience/"")"
"0","# DorsalAttn_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/DorsalAttn/"")"
"0","# FrontoParietal_es <- vertex_level_es(""/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/FrontoParietal/"")"
"0","# "
"0","# df <- data.frame (network  = c(rep(""RetrosplenialTemporal"", length(RetrosplenialTemporal_es)),"
"0","#                                rep(""Auditory"", length(Auditory_es)),"
"0","#                                rep(""SMmouth"", length(SMmouth_es)),"
"0","#                                rep(""SMhand"", length(SMhand_es)),"
"0","#                                rep(""Visual"", length(Visual_es)),"
"0","#                                rep(""None"", length(None_es)),"
"0","#                                rep(""Default"", length(Default_es)),"
"0","#                                rep(""CinguloParietal"", length(CinguloParietal_es)),"
"0","#                                rep(""VentralAttn"", length(VentralAttn_es)),"
"0","#                                rep(""CinguloOperc"", length(CinguloOperc_es)),"
"0","#                                rep(""Salience"", length(Salience_es)),"
"0","#                                rep(""DorsalAttn"", length(DorsalAttn)),"
"0","#                                rep(""FrontoParietal"", length(FrontoParietal_es))),"
"0","#                   es = c(RetrosplenialTemporal_es, Auditory_es, SMmouth_es, SMhand_es, Visual_es, None_es, Default_es,"
"0","#                          CinguloParietal_es, VentralAttn_es, CinguloOperc_es, Salience_es, DorsalAttn, FrontoParietal_es))"
"0",""
"0","# write.csv(df,""/Users/peetal/Documents/GitHub/WMBW/data/Gordon_vertex_es.csv"", row.names = F)"
"0",""
"0","gordon_vertex <- import(here::here(""data/Gordon_vertex_es.csv""))"
"0","# get ready for plotting"
"0","colors = c(""wheat1"",""darkorchid1"",""orange"",""cyan"",""blue"",""snow2"",""chartreuse1"","
"0","           ""pink"",""black"",""aquamarine3"",""purple3"",""red"",""yellow"") # match the color of the brain network"
"0","gordon_vertex %>%"
"0","  mutate(network_name = as.factor(gordon_vertex$network)) %>%"
"0","  ggplot(aes(x = network, y = es)) + "
"0","  geom_violin(fill = ""gray"") + "
"0","  geom_boxplot(width = 0.2, fill = ""white"") +"
"0","  theme(legend.position = ""none"") +"
"0","  scale_y_continuous(breaks = c(-1,-0.5,0,0.5,1,1.5)) + "
"0","  scale_x_discrete(limits=c(""RetrosplenialTemporal"",""Auditory"","
"0","                            ""SMmouth"",""SMhand"",""Visual"",""None"","
"0","                            ""Default"",""CinguloParietal"",""VentralAttn"","
"0","                            ""CinguloOperc"",""Salience"",""DorsalAttn"",""FrontoParietal"""
"0","                            )) +"
"0","  ylim(c(-1,1.5)) +"
"0","  coord_flip() +"
"0","  geom_hline(yintercept = 0.8, linetype = ""dashed"", color = ""red"") +"
"0","  annotate(""text"", x = 3, y = 1.1, label = ""d = 0.8"", color = ""red"") +"
"0","  theme_minimal() +"
"0","  theme(axis.title.x = element_text(size = 10, face =""bold""),"
"0","        axis.text.x = element_text(size = 10, face = ""bold"", margin = margin(t = 0, r = 0, b = 10, l = 0)),"
"0","        axis.title.y = element_text(size = 10, face = ""bold"", margin = margin(t = 0, r = 10, b = 0, l = 0)),"
"0","        axis.text.y = element_text(size = 10, color = colors,face = ""bold""), "
"0","        plot.margin = margin(0,0,0,0,""cm"")) +"
"0","  ylab(""Effect Size"") +"
"0","  xlab(""Gordon Networks"") #+"
"2","Scale for 'y' is already present. Adding another scale for 'y', which will replace the existing scale.
"
