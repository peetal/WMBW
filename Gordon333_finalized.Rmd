---
title: "WMBW_Gordon333"
output: html_document
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE,message=FALSE}
library(rio)
library(tidyverse)
library(sjmisc)
library(DescTools)
library(tidymodels)
library(janitor)
library(magrittr)
library(doParallel)
library(kableExtra)
#library(Rmisc)
library(afex)
library(cocor)
library(psych)
```

### Check and remove potential outliers (behavioral and neural)
```{r outlier_check}
rm(list = ls())

# load in the COPE11 data with Gordon333 parcellation scheme applied:
gordon333_cope11 <- import(here::here('./data/wm_gordon_cop11_pe_2mm_333.csv'))
# load in behavioral results (2-back, list-sorting, Pmat, picVocalb, reading_recog)
behavioral <- import(here::here('./data/HCP_behavioral_data.csv')) %>%
  select(Subject, WM_Task_2bk_Acc, ListSort_AgeAdj, PMAT24_A_CR, PicVocab_AgeAdj, ReadEng_AgeAdj)
# merge and remove na
working_df <- inner_join(gordon333_cope11, behavioral, by = 'Subject') %>% 
  na.omit()

# subjects who had more than 20 parcels showing extreme values. 
lower_threshold <- 
  working_df[,2:ncol(gordon333_cope11)] %>%
  map_dbl(~ quantile(., 0.25) - 1.5*IQR(.)) %>% unname()
upper_threshold <-  
  working_df[,2:ncol(gordon333_cope11)] %>%
  map_dbl(~ quantile(., 0.75) + 1.5*IQR(.)) %>% unname()

working_df_copy1 <-data.frame(working_df)
for (col in 2:ncol(gordon333_cope11)){
  working_df_copy1[,col] <- ifelse((working_df_copy1[,col] < lower_threshold[col-1]) | (working_df_copy1[,col] > upper_threshold[col-1]), 1, 0)
}

outlier_index_neural <- 
  working_df_copy1[,1:ncol(gordon333_cope11)] %>% 
  pivot_longer(., L_Auditory_ID10: R_Visual_ID311, names_to = "parcel_name",  values_to = "value") %>%
  filter(value == 1) %>%
  count(Subject) %>% 
  filter(n > 40)

# subjects who had extreme value in any of the behavioral measure.
lower_threshold <- 
  working_df[,(ncol(gordon333_cope11)+1):ncol(working_df)] %>%
  map_dbl(~ quantile(., 0.25) - 1.5*IQR(.)) %>% unname()
upper_threshold <-  
  working_df[,(ncol(gordon333_cope11)+1):ncol(working_df)] %>%
  map_dbl(~ quantile(., 0.75) + 1.5*IQR(.)) %>% unname()

working_df_copy2 <-data.frame(working_df)
for (col in (ncol(gordon333_cope11)+1):ncol(working_df)){
  working_df_copy2[,col] <- ifelse((working_df_copy2[,col] < lower_threshold[col-334]) | (working_df_copy2[,col] > upper_threshold[col-334]), 1, 0)
}

outlier_index_behavioral <- 
  working_df_copy2 %>% 
  select(Subject, WM_Task_2bk_Acc, ListSort_AgeAdj, PMAT24_A_CR, PicVocab_AgeAdj, ReadEng_AgeAdj) %>%
  pivot_longer(., WM_Task_2bk_Acc: ReadEng_AgeAdj, names_to = "parcel_name",  values_to = "value") %>%
  filter(value == 1) %>%
  count(Subject)

outlier_index_behavioral

# index the good subjects 
#good_sub <- working_df$Subject[!(working_df$Subject %in% outlier_index_neural$Subject)]
# rewrite the neural data: 
#gordon333_cope11 %>% 
#  filter(Subject %in% good_sub) %>%
#  write.csv(., file = "./data/gordon333_cope11_rm_outlier.csv", row.names = FALSE)
```

### Parcels have larger effect sizes
```{r parcel_es, warning = F}
rm(list = ls())

# load in the COPE11 data with gordon400 parcellation scheme applied:
gordon333_cope11 <- import(here::here('./data/gordon333_cope11_rm_outlier.csv')) %>%
  select(-Subject) # dont care about subject id here

# set up parameters: 
network_suffix <- c("Auditory","CinguloOperc","CinguloParietal","Default",
       "DorsalAttn","FrontoParietal","None","RetrosplenialTemporal",
       "Salience","SMhand","SMmouth","VentralAttn","Visual")
cohenD <- c() # Each parcel's load effect effect size
networks<- c() # Each parcel's network 
num <- c() # count the number of parcels in each network 
for (i in network_suffix) { # i <- 'Cont'
  num <- append(num, ncol(gordon333_cope11[,grep(i,colnames(gordon333_cope11))])) # number of parcels in each network
  cohenD <- append(cohenD, gordon333_cope11[,grep(i,colnames(gordon333_cope11))] %>%
                     map(~ (mean(.)/sd(.)))) # effect size for each parcel mean of the effect over sd of the data (Poldrack et al., 2017) 
  networks <- append(networks, rep(i,ncol(gordon333_cope11[,grep(i,colnames(gordon333_cope11))]))) # network names 
}

# create a data frame that has network and effect_size. 
info_table <- data.frame(array(data=NA, dim = c(333,2)))
colnames(info_table) <- c("network_name","effect_size")
info_table$effect_size <- as.numeric(cohenD)
info_table$network_name <- networks

# change the name for network for plotting purpose
for (i in 1: length(network_suffix)) {
  info_table$network_name <- gsub(pattern = network_suffix[i], replacement = paste0(network_suffix[i],"(",num[i],")"), x = info_table$network_name)
}

# get ready for plotting
colors = c("wheat1","darkorchid1","orange","cyan","blue","snow2","chartreuse1",
           "pink","black","aquamarine3","purple3","red","yellow") # match the color of the brain network
info_table %>%
  mutate( network_name = as.factor(info_table$network_name),
         network_name = reorder(info_table$network_name, info_table$effect_size, FUN = mean)) %>%
  ggplot(aes(network_name, effect_size)) + 
  geom_violin(fill = "gray") + 
  geom_boxplot(width = 0.2, fill = "white") +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = c(-1,-0.5,0,0.5,1,1.5)) + 
  scale_x_discrete(limits=c("RetrosplenialTemporal(8)","Auditory(24)",
                            "SMmouth(8)","SMhand(38)","Visual(39)","None(47)",
                            "Default(41)","CinguloParietal(5)","VentralAttn(23)",
                            "CinguloOperc(40)","Salience(4)","DorsalAttn(32)","FrontoParietal(24)"
                            )) +
  coord_flip() +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red") +
  annotate("text", x = 3, y = 1.1, label = "d = 0.8", color = "red") +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 10, face ="bold"),
        axis.text.x = element_text(size = 10, face = "bold", margin = margin(t = 0, r = 0, b = 10, l = 0)),
        axis.title.y = element_text(size = 10, face = "bold", margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.text.y = element_text(size = 10, color = colors,face = "bold"), 
        plot.margin = margin(0,0,0,0,"cm")) +
  ylab("Effect Size") +
  xlab("Gordon Networks") #+ 
  #ggsave("plots/suppmat/gordon_es_parcel.svg", width=4.5, height=3)

# Stats
info_table %>%
  mutate(network_name = as.factor(network_name)) %>%
  group_by(network_name) %>%
  summarize(mean_effect_size = mean(effect_size),
            sd_effect_size = sd(effect_size))

```

```{r vertex_es, warning = F}
# -----------------------------------
# get vertex index for each network 
# -----------------------------------
# rm(list=ls())
# 
# in_path <- "/Users/peetal/Documents/Honor_project/GordonParcelTemplate/Parcels/"
# out.path <- "/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/";
# g.key <- read.csv("/Users/peetal/Documents/Honor_project/GordonParcelTemplate/Parcels.csv", stringsAsFactors=FALSE); # which parcels in each community; 
# setwd("/Users/peetal/Documents/Honor_project/workbench/bin_macosx64/")
# 
# # make a text version of the Gordon atlas - parcel assignment for each vertex
# dt.fname <- paste0(out.path, "Gordon_Parcels_LR.dtseries.txt");
# if (!file.exists(dt.fname)) { 
#   in.fname <- paste0(in_path, "Parcels_LR.dtseries.nii");     # part of the Gordon atlas download
#   if (!file.exists(in.fname)) { stop(paste("missing:", in.fname)); }
#   system(paste0("./wb_command -cifti-convert -to-text ", in.fname, " ", dt.fname));   # call the  wb_command function
# }
# in.dt <- read.table(dt.fname)[,1];   # just one column in the file
# 
# # each community has a different number of component parcels, and vertices in each parcel.
# # make a single file for each community listing *all* of its vertices.
# networks <- c("RetrosplenialTemporal","Auditory","SMmouth","SMhand","Visual","None","Default",
#               "CinguloParietal","VentralAttn","CinguloOperc","Salience","DorsalAttn","FrontoParietal")
# for (do.comm in networks) {   
#   for (do.hem in c("L", "R")) {   # do.comm <- "Visual"; do.hem <- "L";
#     need.parcels <- g.key$ParcelID[which(g.key$Community == do.comm & g.key$Hem == do.hem)];  # parcels for this community & hemisphere
#     
#     # vertices (cifti rows) for each of these parcels
#     fout <- file(paste0(out.path, do.comm, "_", do.hem, "_ciftirows.txt"), 'wt');   # start a blank text file for writing
#     for (pid in 1:length(need.parcels)) {   # pid <- 1;
#       inds <- which(in.dt == need.parcels[pid]);   # rows of the cifti for parcel need.parcel[pid] only
#       for (i in 1:length(inds)) { cat(inds[i], file=fout, sep="\n"); }    # write out a line with each row number
#     }
#     close(fout); unlink(fout);    # let go of the file
#   }
# }
# 
# # -------------------------------------------------------------------------
# # get vertex level neural data (COPE11) for each subject, for each network 
# # -------------------------------------------------------------------------
# 
# # important parameters
# in_path <- "/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/";   # _ciftirows.txt files, cope and id keys, etc.
# out_path <- "/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/";   # files written out to here                       
# comm_ids <- networks; 
# hem_ids <- c("L", "R");   
# 
# 
# # subject IDs
# ids <- import(here::here('./data/gordon333_cope11_rm_outlier.csv')) %>%
#   select(Subject) %>% unname() %>% unlist()
# missing_id <- 0
# 
# 
# for (cid in 1:length(comm_ids)) { #for each network #cid = 1
#   if (!dir.exists(paste0(in_path, "Gordon/",comm_ids[cid]))){ # if sub-folders for the network does not exist 
#     dir.create(paste0(in_path, "Gordon/",comm_ids[cid]))
#   }
#   for (sub in 1:length(ids)) { #for each subject #sub = 2
#     
#     vertex_fname <- paste0(out_path, "cope11_vertex_dat/", ids[sub], "_s1200_WMcope11s.txt")
#     if (!file.exists(vertex_fname)) { missing_id <- missing_id + 1; next; }
#     
#     sub_L_R <- c()
#     for (hid in 1:length(hem_ids)) {   # for each hemisphere hid = 1;
#       # get the rows of the text-ized cifti for this community and hemisphere
#       fname <- paste0(in_path, "Gordon/", comm_ids[cid], "_", hem_ids[hid], "_ciftirows.txt");  
#       if (!file.exists(fname)) { stop(paste("missing:", fname)); }
#       need_rows <- read.table(fname)[,1];   # read in the file, just keeping the first column
#       
#       # get COPE data:
#       vertex <- read.table(vertex_fname)[,1]
#       sub_L_R <- append(sub_L_R, vertex[need_rows])
#     }
#     
#     # write out data
#     sub_out_tbl <- array(NA, c(1, length(sub_L_R)));   # blank table to hold the values we need to keep
#     rownames(sub_out_tbl) <- "cope11";
#     colnames(sub_out_tbl) <- paste0("v", 1:length(sub_L_R));
#     sub_out_tbl[1,] <- sub_L_R
#     out_fname <- paste0(out_path, "Gordon/", comm_ids[cid], "/", ids[sub], "_", comm_ids[cid], "_cope11s.txt")
#     write.table(sub_out_tbl, out_fname)
#   }
# }
# 
# # --------------------
# # compute effect size
# # --------------------
# 
# # function that computes vertex level effect size for a given network
# vertex_level_es <- function(dir) { # dir <- "/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/FrontoParietal/"
#   
#   data_from_dir <- Sys.glob(paste0(dir, '*.txt')) # all vertex within this network
#   nVertex <- ncol(read.table(data_from_dir[1])) # number of vertices in this network
#   nSub <- length(data_from_dir) # number of subjects 
#   network_vertex_data <- data.frame(array(NA, c(nSub, nVertex + 1))) # create an empty dataframe
#   
#   for (i in 1:length(data_from_dir)){ # i = 1
#   
#     sub_id <- unlist(str_split(unlist(str_split(data_from_dir[i], "/"))[10],"_"))[1] # subject id
#     network_vertex_data[i,1] <- sub_id # add subject id to the dataframe
#   
#     sub_data <- read.table(data_from_dir[i]) # extract in neural data 
#     network_vertex_data[i,1:nVertex + 1] <- sub_data[1,] %>% unname() %>% unlist() # extract in neural data 
#   }                                 
#   
#   # compute load effect size for all the vertices
#   cohenD <- network_vertex_data %>%
#     select(-X1) %>%
#     map_dbl(~ (mean(.)/sd(.))) # effect size for each parcel mean of the effect over sd of the data (Poldrack et al., 2017) 
#   
#   return(cohenD)
# }
# 
# 
# RetrosplenialTemporal_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/RetrosplenialTemporal/")
# Auditory_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Auditory/")
# SMmouth_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/SMmouth/")
# SMhand_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/SMhand/")
# Visual_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Visual/")
# None_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/None/")
# Default_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Default/")
# CinguloParietal_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/CinguloParietal/")
# VentralAttn_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/VentralAttn/")
# CinguloOperc_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/CinguloOperc/")
# Salience_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/Salience/")
# DorsalAttn_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/DorsalAttn/")
# FrontoParietal_es <- vertex_level_es("/Users/peetal/Documents/Honor_project/Schaefer/vertex_level_data/Gordon/FrontoParietal/")
# 
# df <- data.frame (network  = c(rep("RetrosplenialTemporal", length(RetrosplenialTemporal_es)),
#                                rep("Auditory", length(Auditory_es)),
#                                rep("SMmouth", length(SMmouth_es)),
#                                rep("SMhand", length(SMhand_es)),
#                                rep("Visual", length(Visual_es)),
#                                rep("None", length(None_es)),
#                                rep("Default", length(Default_es)),
#                                rep("CinguloParietal", length(CinguloParietal_es)),
#                                rep("VentralAttn", length(VentralAttn_es)),
#                                rep("CinguloOperc", length(CinguloOperc_es)),
#                                rep("Salience", length(Salience_es)),
#                                rep("DorsalAttn", length(DorsalAttn)),
#                                rep("FrontoParietal", length(FrontoParietal_es))),
#                   es = c(RetrosplenialTemporal_es, Auditory_es, SMmouth_es, SMhand_es, Visual_es, None_es, Default_es,
#                          CinguloParietal_es, VentralAttn_es, CinguloOperc_es, Salience_es, DorsalAttn, FrontoParietal_es))

# write.csv(df,"/Users/peetal/Documents/GitHub/WMBW/data/Gordon_vertex_es.csv", row.names = F)

gordon_vertex <- import(here::here("data/Gordon_vertex_es.csv"))
# get ready for plotting
colors = c("wheat1","darkorchid1","orange","cyan","blue","snow2","chartreuse1",
           "pink","black","aquamarine3","purple3","red","yellow") # match the color of the brain network
gordon_vertex %>%
  mutate(network_name = as.factor(gordon_vertex$network)) %>%
  ggplot(aes(x = network, y = es)) + 
  geom_violin(fill = "gray") + 
  geom_boxplot(width = 0.2, fill = "white") +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = c(-1,-0.5,0,0.5,1,1.5)) + 
  scale_x_discrete(limits=c("RetrosplenialTemporal","Auditory",
                            "SMmouth","SMhand","Visual","None",
                            "Default","CinguloParietal","VentralAttn",
                            "CinguloOperc","Salience","DorsalAttn","FrontoParietal"
                            )) +
  ylim(c(-1,1.5)) +
  coord_flip() +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red") +
  annotate("text", x = 3, y = 1.1, label = "d = 0.8", color = "red") +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 10, face ="bold"),
        axis.text.x = element_text(size = 10, face = "bold", margin = margin(t = 0, r = 0, b = 10, l = 0)),
        axis.title.y = element_text(size = 10, face = "bold", margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.text.y = element_text(size = 10, color = colors,face = "bold"), 
        plot.margin = margin(0,0,0,0,"cm")) +
  ylab("Effect Size") +
  xlab("Gordon Networks") #+
  #ggsave("plots/suppmat/gordon/gordon_es_vertex.svg", width=4.5, height=3)

gordon_vertex %>%
  mutate(network = as.factor(network)) %>%
  group_by(network) %>%
  summarize(mean_effect_size = mean(es),
            sd_effect_size = sd(es))

```

### Identification of WM-involved networks.
```{r distinct_network}
rm(list = ls())

# read in neural and behavioral data
gordon333_cope11 <- import(here::here('./data/gordon333_cope11_rm_outlier.csv'))
behavioral <- import(here::here('./data/HCP_behavioral_data.csv')) %>%
  select(Subject, WM_Task_2bk_Acc)
# join 
working_df <- inner_join(gordon333_cope11, behavioral, by = 'Subject') %>% 
  na.omit()

# ------------------------------------------------------------------------------
# to identify brain network that is sensitive to within-subject variations
# ------------------------------------------------------------------------------
# do a one sample t test for each parcel to identify within network
within_ttest <- 
  working_df %>%
  select(-c(Subject,WM_Task_2bk_Acc)) %>%
  map(~t.test(., mu = 0, alternative = "two.sided"))

# load activated parcel in the within network
load_activated_parcel_ttest <- 
  within_ttest[within_ttest %>% map(~.$statistic) > 0]
load_activated_parcel <- 
  load_activated_parcel_ttest[load_activated_parcel_ttest %>% map(~.$p.value) < 0.05/length(within_ttest)] %>%
  names()

# load deactivated parcel in the within network
load_deactivated_parcel_ttest <- 
  within_ttest[within_ttest %>% map(~ .$statistic) < 0]
load_deactivated_parcel <- 
  load_deactivated_parcel_ttest[load_deactivated_parcel_ttest %>% map(~.$p.value) < 0.05/length(within_ttest)] %>%
  names()

# load insensitive parcel 
load_insensitive_parcel <- colnames(gordon333_cope11)[-1][! colnames(gordon333_cope11)[-1] %in% c(load_activated_parcel,load_deactivated_parcel)]

# ------------------------------------------------------------------------------
# to identify brain network that is sensitive to between-subject variations
# ------------------------------------------------------------------------------

# parcels activation correlates with behavioral performance (between-network)
between_cortest_list <- 
  working_df %>%
  select(-c(Subject, WM_Task_2bk_Acc)) %>%
  map(~cor.test(., working_df$WM_Task_2bk_Acc))
  
# parcels that are positively correlated with behavior
beh_pos_cor_parcel_ttest <-
  between_cortest_list[between_cortest_list %>% map(~ magrittr::extract2(., 4)) > 0]
beh_pos_cor_parcel_name <-
  beh_pos_cor_parcel_ttest[beh_pos_cor_parcel_ttest %>% map(~ magrittr::extract2(., 3)) < 0.05/length(between_cortest_list)] %>%
  names()

# parcels that are negatively correlated with behavior
beh_neg_cor_parcel_ttest <-
  between_cortest_list[between_cortest_list %>% map(~ magrittr::extract2(., 4)) < 0]
beh_neg_cor_parcel_name <-
  beh_neg_cor_parcel_ttest[beh_neg_cor_parcel_ttest %>% map(~ magrittr::extract2(., 3)) < 0.05/length(between_cortest_list)] %>%
  names()

# --------------------------------------------------------
# Check the consistency of the directions of the effects. 
# --------------------------------------------------------
# out of 137 parcels exhibiting between-subjects WM effects, how many of them also 
# sensitive to within-subject difference? 
# n = 136
sum(c(beh_pos_cor_parcel_name,beh_neg_cor_parcel_name)  %in% c(load_activated_parcel, load_deactivated_parcel))
# out of 91 parcels showing positive behavioral correlation, how many are load-activated?
# n = 87
sum(beh_pos_cor_parcel_name %in% load_activated_parcel)
# out of 46 parcels showing negative behavioral correaltion, how many are load-deactivated?
# n = 46
sum(beh_neg_cor_parcel_name %in% load_deactivated_parcel)

```

### Parcels contribute to between and within-subject variations equivalently. 
```{r compare_es,warning=FALSE,message=FALSE}

# get cohenD and correlation coefficient ranking for every parcel: 
es_info <- data.frame(array(data=NA, dim = c(333,3)))
colnames(es_info) <- c('parcel_names','cohenD','coref') # colun names 
es_info$parcel_names <- colnames(working_df)[2:334] # parcel names 
es_info$cohenD <- working_df[,2:334] %>% map_dbl(~ (mean(.)/sd(.))) # compute cohen's D 
es_info$coref <- 
  working_df[,2:334] %>% 
  map_dbl(~ (cor.test(.,working_df$WM_Task_2bk_Acc)$estimate)) # compute correaltion with behavior. 

# rank the absolute values of the two effect sizes. 
es_info <- es_info %>%
  mutate(rank_cohenD = 334 - rank(abs(es_info$cohenD)),
         rank_coref = 334 - rank(abs(es_info$coref)),
         Parcel_type = ifelse(es_info$parcel_names %in% load_activated_parcel, 'Activated', ifelse(es_info$parcel_names %in% load_deactivated_parcel, 'Deactivated','Insensitive')))

# get all parcels names in either load-activated network or between network, plot the two rank for these parcels
colors = c("red","blue","grey")
es_info %>%
  ggplot(aes(x = rank_cohenD, y = rank_coref, color = Parcel_type)) + 
  facet_grid(cols = vars(Parcel_type), scales = 'free') + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = 'lm', se = F) + 
  scale_color_manual(values = colors) + 
  theme_minimal() +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10),
        axis.text.y = element_text(size = 6), 
        strip.text.x = element_text(size = 8),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.title = element_blank(),
        legend.position = 'None') +
  xlab("Within-subject effect rank \n(rank of |Cohen's d|)") +
  ylab("Between-subject effect rank \n(rank of |r|)") #+
  #ggsave("plots/suppmat/gordon/rank_cor.svg", width=3.3, height=2.8)

# --------------------------------------------------------------------------------------
# compute Spearman correlation coefficient of the rank orders of the two effect sizes. 
# for 3 functional groups separately. 
# --------------------------------------------------------------------------------------
within_between_es <- 
  es_info %>%
  group_by(Parcel_type) %>%
  nest() %>%
  mutate(cortest = map(data, ~ cor.test(.$rank_cohenD, .$rank_coref, data = ., method = "spearman")),
         estimate = map_dbl(cortest, ~ .$estimate),
         p_value = map_dbl(cortest, ~ .$p.value)) %>% 
  select(Parcel_type, estimate, p_value)
within_between_es

# -----------------------------------------
# z test for the correlation coefficients. 
# -----------------------------------------
parcel_count <- 
  es_info %>%
    group_by(Parcel_type) %>%
    summarize(count = n())
r_activated <- within_between_es$estimate[2]
n_activated <- parcel_count$count[1]
r_deactivated <- within_between_es$estimate[1]
n_deactivated <- parcel_count$count[2]

# is the coupling of the two effect sizes stronger for load-activated parcels? 
z_diff <- atanh(r_activated) - atanh(r_deactivated)
z_se <- ((1/(n_activated-3))+(1/(n_deactivated-3)))^0.5
print(z_diff/z_se)
2*(1-pnorm(z_diff/z_se))

# ----------------------------------------------------------------
# Descriptive stats for individual difference sensitive parcels
# ----------------------------------------------------------------
es_info %>% filter(parcel_names %in% beh_pos_cor_parcel_name) %>%select(coref) %>% describe(.)
es_info %>% filter(parcel_names %in% beh_neg_cor_parcel_name) %>%select(coref) %>% describe(.)

```

### Network distribution of within- and between-subject effect biased regions
```{r}
# get index of whether a parcel is biased toward within- or between-subject differences 
bias_info <-  es_info %>%
  mutate(rank_diff = rank_coref - rank_cohenD) %>%
  mutate(rank_diff = scale(rank_diff)) %>%
  select(parcel_names, rank_diff)

# get the network name and the subnetwork name
bias_info$network <- bias_info$parcel_names %>% 
  map(~ str_split(., "_") %>% 
        magrittr::extract2(., 1)) %>% 
  map(~ .[2]) %>% unlist()

# group by only main network 
bias_table <- 
  bias_info %>% 
  group_by(network) %>%
  summarise(Within_subject_effect_biased = sum(rank_diff >= 0),
            Between_subject_effect_biased = sum(rank_diff < 0))

# chi-square test
test_bias <- function(network_name, bias_table){
  net <- c(bias_table$Within_subject_effect_biased[which(bias_table$network == network_name)], 
           bias_table$Between_subject_effect_biased[which(bias_table$network == network_name)])
  print(paste0("testing ", network_name, " network"))
  return(chisq.test(net))
}
bias_table$network %>% map(~ test_bias(., bias_table))

# dataframe into table 
rownames(bias_table) <- NULL
colnames(bias_table) <- c("Network", "Within-subject Variation \n Biased Region Count", "Between-subject Variation \n Biased Region Count")
bias_table %>%
  kbl(booktabs = T, align = "c") %>%
  kable_classic_2(full_width = F) %>%
  column_spec(., 2:3, width = "4cm") 
```

### Computing ICC
```{r, eval = F}
contrast <- import('data/gordon333_cope11_rm_outlier.csv')
subj <- contrast$Subject
nback_2b <- import('/Users/peetal/Documents/Honor_project/data/Gordon/2mm/WM/cope9_pe/WM_Surf_2mm_cope9_pe.csv') %>%
  filter(Subject %in% subj)
nback_0b <- import('/Users/peetal/Documents/Honor_project/data/Gordon/2mm/WM/cope10_pe/WM_Surf_2mm_cope10_pe.csv') %>%
  filter(Subject %in% subj)

compute_icc <- function(parcel){ #parcel = 'L_Auditory_ID10'
  parcel_2b <- nback_2b %>% select(parcel) %>% unname() %>% unlist()
  parcel_0b <- nback_0b %>% select(parcel) %>% unname() %>% unlist()
  merge <- data.frame(parcel_0b, parcel_2b)
  
  computed_icc <- psych::ICC(merge)
  icc3k <- computed_icc$results %>% filter(type == 'ICC1') %>% select(ICC) %>% as.numeric()
}

parcel_names <- es_info$parcel_names
icc_scores <- c()
for (parcel in parcel_names){
  icc_scores <- append(icc_scores, compute_icc(parcel))
}

es_info$icc = icc_scores
es_info$rank_icc <- 334 - rank(icc_scores)

within_icc_es <- 
  es_info %>%
  group_by(Parcel_type) %>%
  nest() %>%
  mutate(cortest = map(data, ~ cor.test(.$rank_cohenD, .$rank_icc, data = ., method = "spearman")),
         estimate = map_dbl(cortest, ~ .$estimate),
         p_value = map_dbl(cortest, ~ .$p.value)) %>% 
  select(Parcel_type, estimate, p_value)
within_icc_es

cor.test(es_info$rank_cohenD, es_info$rank_icc)
hist(es_info$icc)

colors = c("red","blue","grey")
es_info %>%
  ggplot(aes(x = rank_cohenD, y = rank_icc)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = 'lm', se = F, color = 'black') + 
  theme_minimal() +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        axis.text.y = element_text(size = 10), 
        strip.text.x = element_text(size = 10),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.title = element_blank(),
        legend.position = 'None') +
  xlab("rank of |Cohen's d|") +
  ylab("rank of ICC") + 
  ggsave(paste0("plots/suppmat/gordon/cohend_icc.svg"), width=3.5, height=2.5)



es_info %>%
  ggplot(aes(x = rank_coref, y = rank_icc)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = 'lm', se = F, color = 'black') + 
  theme_minimal() +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        axis.text.y = element_text(size = 10), 
        strip.text.x = element_text(size = 10),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.title = element_blank(),
        legend.position = 'None') +
  xlab("rank of |r|") +
  ylab("rank of ICC") + 
  ggsave(paste0("plots/suppmat/gordon/pearsonr_icc.svg"), width=3.5, height=2.5)



#output <- es_info %>% select(parcel_names, icc) %>% write.csv(., file = '/Users/peetal/Desktop/gordon_icc.csv', row.names = F)

df <- import(here::here(paste0("data/gordon_permutation/", "gordon_feature_selection_permutation_WM_Task_2bk_Acc.csv")))
df$fs <- c(0.00944725322781218,
 -0.05361030702088929,
 0.11411127730427924,
 0.12656267453389092,
 0.1240426799316845,
 0.15381346710007268,
 0.153974790100809,
 0.20494283601620703,
 0.22740154521422803,
 0.23353770910906874,
 0.22636191332907202,
 0.23600301239938984,
 0.2576437559915502,
 0.2818439990945649,
 0.32195776071800797,
 0.32052841160159395,
 0.32841771193420033,
 0.3621364720274464,
 0.35307980613739165,
 0.35459927540167996,
 0.3622489216381649,
 0.36285357670458934,
 0.35867124206254913,
 0.3624093449370035,
 0.36900632062346517,
 0.366742278090206,
 0.3671768754683925,
 0.3797976691440927,
 0.38022864608715945,
 0.3835091891672183,
 0.3816371152957173,
 0.38450279540652577,
 0.3880738462600139,
 0.38946277986185057,
 0.4014369095880668,
 0.40150448629857366,
 0.4022506633607631,
 0.39912560648783363,
 0.40072227824443934,
 0.4054254628243855,
 0.40032240357785065,
 0.4095214596354323,
 0.4085154767377621,
 0.4102354760595787,
 0.41084074756208916,
 0.408023534796922,
 0.4275394606350539,
 0.4252647530623147,
 0.42376522361320257,
 0.4268983659450008,
 0.4274085392885759,
 0.42740553671171577,
 0.4223647751870329,
 0.4206729152109987,
 0.41603539379622295,
 0.41610039337795535,
 0.4123659712510195,
 0.4162363578939489,
 0.41457953619251453,
 0.4122643127090444)
  # extract null 
  null <- df %>%
    select(lower, mean, upper, idx)
  # plot
df %>%
    ggplot() + 
    geom_point(aes(x = idx, y = fs), size = 0.6, color = 'red') + 
    geom_point(aes(x = idx, y = observed), size = 0.6, color = 'black') + 
    geom_line(aes(x = idx, y = fs), color = "red", size = 0.5) + 
    geom_line(aes(x = idx, y = observed), color = "black", size = 0.5) +
    geom_point(data = null, aes(x = idx, y = mean), alpha = 0.1, color = "gray1", size = 0.6) + 
    geom_line(data = null, aes(x = idx, y = mean), color = "gray1", alpha = 0.1) +
    geom_ribbon(data = null, aes(x = idx, ymin = lower, ymax = upper), fill = "gray1", 
                linetype = 2, alpha = 0.1, inherit.aes = F) + 
    theme_classic() + 
    ylab("Predictive Acc") + 
    xlab("Number of parcels") + 
    ggtitle(paste0("2-back Task")) + 
    scale_x_continuous(breaks=seq(0, 60, 10))+ 
    theme(axis.title.x = element_text(size = 10),
          axis.text.x = element_text(size = 8),
          plot.title = element_text(size = 10, hjust = 0.5, face = "bold"),
          axis.text.y = element_text(size = 8), 
          strip.text.x = element_text(size = 10),
          plot.margin = margin(0,0,0,0,"cm"),
          legend.position = "none",
          legend.title = element_blank())#+ 
  ggsave(paste0("plots/suppmat/gordon/select_byicc.svg"), width=3.3, height=2.5)



#range(es_info$icc)
```


### Load-effect sizes indicates parcel’s univariate predictive power
```{r}
# -----------------------------------------------------------------------------
# Relationship between load-related effect size and univariate predictive power 
# -----------------------------------------------------------------------------
# univariate pred acc:
schaefer_uni_pred_acc <- import(here::here('data/univariate_pred_acc/gordon_univariate_pred_acc_all_parcel.csv')) %>%
  rename(parcel_names = parcel_name) %>%
  select(parcel_names, pred_wm) 
  
# top and bottom 30 load-act parcels
es_info2_load_act <- inner_join(schaefer_uni_pred_acc, es_info, by="parcel_names") %>% 
  filter(Parcel_type == "Activated") %>%
  arrange(desc(cohenD))

load_act_tail_30 <- 
  es_info2_load_act %>% top_n(30, rank_cohenD) %>% select(parcel_names) %>% unname() %>% unlist() %>% as.vector()
load_act_head_30 <- 
  es_info2_load_act %>% top_n(-30, rank_cohenD) %>% select(parcel_names) %>% unname() %>% unlist() %>% as.vector()
es_info2_load_act <- es_info2_load_act %>%
  filter(parcel_names %in% c(load_act_tail_30, load_act_head_30))
parcel_act_order <- c(load_act_head_30, rep(" ",5), load_act_tail_30)
  
# top and bottom 30 load-deact parcels
es_info2_load_deact <- inner_join(schaefer_uni_pred_acc, es_info, by="parcel_names") %>% 
  filter(Parcel_type == "Deactivated") %>% 
  arrange(cohenD)

load_deact_tail_30 <- 
  es_info2_load_deact %>% top_n(30, rank_cohenD) %>% select(parcel_names) %>% unname() %>% unlist() %>% as.vector()
load_deact_head_30 <- 
  es_info2_load_deact %>% top_n(-30, rank_cohenD) %>% select(parcel_names) %>% unname() %>% unlist() %>% as.vector()
es_info2_load_deact <- es_info2_load_deact %>%
  filter(parcel_names %in% c(load_deact_tail_30, load_deact_head_30))
parcel_deact_order <- c(load_deact_head_30, rep(" ",5), load_deact_tail_30)


es_info2_load_act %>%
  mutate(parcel_names = factor(parcel_names)) %>%
  ggplot(aes(x = parcel_names, y = pred_wm)) + 
  geom_bar(aes(fill = Parcel_type), stat='identity') + 
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_text(size = 10, face = "bold"),
        axis.text.y = element_text(size = 8, face = "bold"), 
        axis.title.x = element_text(size = 10, face = "bold"),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.title = element_blank(),
        legend.position = 'None') +
  scale_x_discrete(limits=parcel_act_order) +
  ylim(-0.12,0.5) + 
  ylab("Predictive power") + 
  xlab("Cohen's d") #+ 
  #ggsave("plots/suppmat/gordon/load_act_univariate.svg", width=3.3, height=1.9)

# correlation between cohenD and univariate predictive power 
cor.test(es_info2_load_act$cohenD, es_info2_load_act$pred_wm)
# mean cohenD and mean univariate predictive power for top and bottom 20 parcels
es_info2_load_act %>%
  filter(rank_cohenD < 40) %>%
  mutate(mean_cohend = mean(cohenD),
         mean_pred_wm = mean(pred_wm)) %>%
  select(mean_cohend, mean_pred_wm) %>% unique()
es_info2_load_act %>%
  filter(rank_cohenD > 40) %>%
  mutate(mean_cohend = mean(cohenD),
         mean_pred_wm = mean(pred_wm)) %>%
  select(mean_cohend, mean_pred_wm) %>% unique()


es_info2_load_deact %>%
  mutate(parcel_names = factor(parcel_names)) %>%
  ggplot(aes(x = parcel_names, y = pred_wm)) + 
  geom_bar(aes(fill = Parcel_type), stat='identity', fill = "dodgerblue3") + 
  theme_classic() +
  theme(axis.title.y = element_text(size = 10, face = "bold"),
        axis.title.x = element_text(size = 10, face = "bold"),
        axis.text.y = element_text(size = 8, face = "bold"), 
        axis.text.x=element_blank(),
        axis.ticks.x = element_blank(),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.title = element_blank(),
        legend.position = 'None') +
  scale_x_discrete(limits=parcel_deact_order) +
  ylim(-0.12,0.5) + 
  ylab("Predictive power") + 
  xlab("Cohen's d") #+ 
  #ggsave("plots/suppmat/gordon/load_deact_univariate.svg", width=3.3, height=1.9)

# correlation between cohenD and univariate predictive power 
cor.test(es_info2_load_deact$cohenD, es_info2_load_deact$pred_wm)
# mean cohenD and mean univariate predictive power for top and bottom 20 parcels
es_info2_load_deact %>%
  filter(rank_cohenD < 40) %>%
  mutate(mean_cohend = mean(cohenD),
         mean_pred_wm = mean(pred_wm)) %>%
  select(mean_cohend, mean_pred_wm) %>% unique()
es_info2_load_deact %>%
  filter(rank_cohenD > 40) %>%
  mutate(mean_cohend = mean(cohenD),
         mean_pred_wm = mean(pred_wm)) %>%
  select(mean_cohend, mean_pred_wm) %>% unique()



```

### Generating Table1
```{r}
table <- es_info[order(es_info$rank_cohenD),][1:50,] %>%
  select(parcel_names, cohenD)
# add univariate predictive acc
univariate <- import(here::here('data/univariate_pred_acc/gordon_univariate_pred_acc.csv'))
table$pred_wm_acc <- univariate$pred_wm
#table$pred_ls_acc <- univariate$pred_ls 
# add pracel id 
gordon_order_txt <- import(here::here('data/Gordon333_Key.txt'))
gordon_order_txt$parcel_name <- paste0(gordon_order_txt$Hemisphere, 
                                       '_', gordon_order_txt$Network, '_ID', gordon_order_txt$Gordon_Parcel)
for (row in 1:nrow(table)){ # row = 1
  table$gordon_id[row] <- gordon_order_txt$Gordon_Parcel[which(gordon_order_txt$parcel_name == table$parcel_names[row])]
  #table$icc[row] <- parcel_icc$ICC[which(parcel_icc$parcel == table$parcel_names[row])]
}

# dataframe into table 
rownames(table) <- NULL
colnames(table) <- c("Parcel Name", "Load-effect Size", "Univariate predictive accuracy for 2-back performance", "Gordon ID")
table <- table[c("Parcel Name", "Gordon ID", "Load-effect Size", "Univariate predictive accuracy for 2-back performance")]

table %>%
  kbl(booktabs = T, align = "c") %>%
  kable_classic_2(full_width = F) %>%
  column_spec(., 4, width = "5cm")
```


### Write CIFTI for plotting
```{r}
source('/Users/peetal/Documents/Honor_project/Schaefer/func/write_gordon_cifti.R') 
#  load in order
gordon_order_txt <- import(here::here('data/Gordon333_Key.txt'))
gordon_order <- c()
for (i in 1:nrow(gordon_order_txt)){ # i = 1
  gordon_parcel <- paste0(gordon_order_txt$Hemisphere[i], '_', gordon_order_txt$Network[i], '_ID', gordon_order_txt$Gordon_Parcel[i])
  gordon_order <- append(gordon_order, gordon_parcel)
}

# ---------------------------
# within-subject effect size 
# ---------------------------
outvec <- c()
for (i in gordon_order){ # i = 'LH_Vis_1'
  if (i %in% c(load_activated_parcel, load_deactivated_parcel)){
    outvec <- append(outvec, es_info$cohenD[which(es_info$parcel_names == i)])
  } else{
    outvec <- append(outvec, 0)
  }
}
#write.table(outvec, "/Users/peetal/Documents/Honor_project/Schaefer/working/schaefer400/outvec_txt/gordon_within_subject_effect.txt", col.names=FALSE,row.names=FALSE)
#write_gordon333_pscalar('gordon_within_subject_effect.txt', 'gordon_within_subject_effect.pscalar.nii')

# -----------------------------
# between-subject effects size 
# -----------------------------
outvec <- c()
for (i in gordon_order){ # i = 'LH_Vis_1'
  if (i %in% c(beh_pos_cor_parcel_name, beh_neg_cor_parcel_name)){
    outvec <- append(outvec, es_info$coref[which(es_info$parcel_names == i)])
  } else{
    outvec <- append(outvec, 0)
  }
}
#write.table(outvec, '/Users/peetal/Documents/Honor_project/Schaefer/working/schaefer400/outvec_txt/gordon_between_subject_effect.txt', col.names=FALSE,row.names=FALSE)
#write_gordon333_pscalar('gordon_between_subject_effect.txt', 'gordon_between_subject_effect.pscalar.nii')

# ----------------------------------
# spatial distribution of rank diff
# ----------------------------------
es_info <-  es_info %>%
  mutate(rank_diff = rank_coref - rank_cohenD) %>%
  mutate(rank_diff = scale(rank_diff))

outvec <- c()
for (i in gordon_order){ # i = 'LH_Vis_1'
  outvec <- append(outvec,es_info[which(es_info$parcel_names == i),]$rank_diff)
}

#write.table(outvec, '/Users/peetal/Documents/Honor_project/Schaefer/working/schaefer400/outvec_txt/gordon_parcels_es_diff.txt', col.names=FALSE,row.names=FALSE)
#write_gordon333_pscalar('gordon_parcels_es_diff.txt', 'gordon_parcels_es_diff.pscalar.nii')


# ---------------
# top 30 parcels: 
# ---------------
top_30_parcels <- es_info[order(es_info$rank_cohenD),][1:30,] %>% select(parcel_names) %>% unlist() %>% unname()
top_5_parcels <- top_30_parcels[1:5]

outvec <- c()
for (i in gordon_order){ # i = 'LH_Vis_1'
  if (i %in% top_30_parcels){
    if (i %in% top_5_parcels){
      outvec <- append(outvec,2)
    } else {
      outvec <- append(outvec,1)
    }
  } else{
    outvec <- append(outvec,0)
  }
}

#write.table(outvec, '/Users/peetal/Documents/Honor_project/Schaefer/working/schaefer400/outvec_txt/gordon_top_30.txt', col.names=FALSE,row.names=FALSE)
#write_gordon333_pscalar('gordon_top_30.txt', 'gordon_top_30.pscalar.nii')

```

### Load-effect sizes indicates parcel's predictive power
```{r nested_permutation}
rm(list = ls())
# plotting for predictive power per parcel bins: 
plot_pred_acc_bin <- function(df_name, measure){
  # load dataframe
  df_inscanner <- import(here::here(paste0("data/gordon_permutation/",df_name)))
  # select real_data
  real_data <- 
    df_inscanner[1,] %>% 
    pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
                 values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
    mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Deactivated"))
  # plotting 
  real_data %>% 
    ggplot(aes(x = effect_size, y = pred_acc, color = parcel_type)) +
    geom_point() + 
    geom_line(aes(group = parcel_type, color = parcel_type)) + 
    geom_hline(yintercept = df_inscanner$load_insensitive[1], color = "gray", linetype = "dashed") + 
    ylim(0, df_inscanner$act_13_15[1] + 0.1) + 
    xlab("") + 
    ylab("Predictive Acc") + 
    ggtitle(paste0(measure)) + 
    scale_x_discrete(labels=c("01_03" = "(0.1,0.3)", "03_05" = "(0.3,0.5)", "05_07" = "(0.5,0.7)", 
                              "07_09" = "(0.7,0.9)", "09_11" = "(0.9,1.1)", "11_13" = "(1.1,1.3)",
                              "13_15" = "(1.3,1.5)")) + 
    theme_classic() +
    theme(axis.title.x = element_text(size = 10),
          axis.text.x = element_text(size = 6),
          axis.title.y = element_text(size = 10),
          axis.text.y = element_text(size = 8), 
          plot.title = element_text(size = 10, hjust = 0.5, face = "bold"),
          plot.margin = margin(0,0,0,0,"cm"),
          legend.position = c(0.2, 0.8)) #+ 
    #ggsave(paste0("plots/suppmat/gordon/multivariate_",measure,".svg"), width=3.3, height=1.9)
  
}

# linear trend statistical test: 
linear_trend_stats <- function(df_name){ #df_name = "gordon_nested_permutation_test_12bin_WM_Task_2bk_Acc.csv"
  df <- import(here::here(paste0("data/gordon_permutation/",df_name)))
  # select real_data
  real_data <- 
    df[1,] %>% 
    pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
                 values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
    mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Deactivated"))
  # linear trend for activated
  act_real_lt <- real_data %>%
    filter(parcel_type == "Activated") %>%
    mutate(effect_size_rank = c(1,2,3,4,5,6,7)) %>%
    lm(pred_acc ~ effect_size_rank, .) %>% 
    summary()
  # linear trend for deactivated
  deact_real_lt <- real_data %>% 
    filter(parcel_type == "Deactivated") %>%
    mutate(effect_size_rank = c(1,2,3,4)) %>% 
    lm(pred_acc ~ effect_size_rank, .) %>% 
    summary()
  # null linear trend coefficient for activated bins
  null_coef_act <- c()
  for (row in 2:1001){
    mat <- matrix(c(as.numeric(df[row,1:7]), c(1,2,3,4,5,6,7)), ncol = 2)
    null_lt <- lm(mat[,1]~ mat[,2]) %>% summary()
    null_coef_act <- append(null_coef_act, null_lt$coefficients[2,1])
  }
  # null linear trend coefficient for deactivated bins
  null_coef_deact <- c()
  for (row in 2:1001){
    mat <- matrix(c(as.numeric(df[row,8:11]), c(1,2,3,4)), ncol = 2)
    null_lt <- lm(mat[,1]~ mat[,2]) %>% summary()
    null_coef_deact <- append(null_coef_deact, null_lt$coefficients[2,1])
  }
  # compute p-values for the observed linear trend coefficients. 
  #act_real_lt_p <- (1- pnorm((act_real_lt$coefficients[2,1] - mean(null_coef_act))/sd(null_coef_act)))*2
  act_real_lt_p <- sum(act_real_lt$coefficients[2,1] < null_coef_act)/1000
  #deact_real_lt_p <- (1- pnorm((deact_real_lt$coefficients[2,1] - mean(null_coef_deact))/sd(null_coef_deact)))*2
  deact_real_lt_p <- sum(deact_real_lt$coefficients[2,1] < null_coef_deact)/1000
  
  print(paste0("Act linear trend coefficient is ", act_real_lt$coefficients[2,1], "and its P values is ", act_real_lt_p))
  print(paste0("Deact linear trend coefficient is ", deact_real_lt$coefficients[2,1], "and its P values is ", deact_real_lt_p))
}

# compare load-activated and load-deactivated parcels given the same effect size. 
compare_pred_acc_bins <- function(df_name){ # df_name = "gordon_nested_permutation_test_12bin_WM_Task_2bk_Acc.csv"
  
  # load data 
  df <- import(here::here(paste0("data/gordon_permutation/",df_name)))
  # extract bins to be compared with 
  real <- data.frame("activated" = as.numeric(df[1,1:4]),
                     "deactivated" = as.numeric(df[1,8:11]),
                     "insensitive" = rep(as.numeric(df[1,12]), 4)) %>%
    mutate(act_minus_deactivated = activated - deactivated,
           deactivated_minus_insensitive = deactivated - insensitive,
           activated_minus_insensitive = activated - insensitive) %>%
    select(act_minus_deactivated, deactivated_minus_insensitive, activated_minus_insensitive)
  
  # build null distribution
  null_01_03 <- c()
  null_03_05 <- c()
  null_05_07 <- c()
  null_07_09 <- c()
  for (row in 2:1001){ # row = 2
      mat <- data.frame("activated" = as.numeric(df[row,1:4]),
                         "deactivated" = as.numeric(df[row,8:11]),
                         "insensitive" = rep(as.numeric(df[row,12]), 4)) %>%
        mutate(act_minus_deactivated = activated - deactivated,
               deactivated_minus_insensitive = deactivated - insensitive,
               activated_minus_insensitive = activated - insensitive) %>%
        select(act_minus_deactivated, deactivated_minus_insensitive, activated_minus_insensitive)
      # build null distribution for the differences of load-activated and deactivated bins
      null_01_03 <- append(null_01_03, mat$act_minus_deactivated[1])
      null_03_05 <- append(null_03_05, mat$act_minus_deactivated[2])
      null_05_07 <- append(null_05_07, mat$act_minus_deactivated[3])
      null_07_09 <- append(null_07_09, mat$act_minus_deactivated[4])
  } 
  
  
  # compute p values: 
  #p_01_03 <- (1- pnorm(abs(real$act_minus_deactivated[1] - mean(null_01_03))/sd(null_01_03)))*2
  p_01_03 <- sum(real$act_minus_deactivated[1] < null_01_03)/1000
  #p_03_05 <- (1- pnorm(abs(real$act_minus_deactivated[2] - mean(null_03_05))/sd(null_03_05)))*2
  p_03_05 <- sum(real$act_minus_deactivated[2] < null_03_05)/1000
  #p_05_07 <- (1- pnorm(abs(real$act_minus_deactivated[3] - mean(null_05_07))/sd(null_05_07)))*2
  p_05_07 <- sum(real$act_minus_deactivated[3] < null_05_07)/1000
  #p_07_09 <- (1- pnorm(abs(real$act_minus_deactivated[4] - mean(null_07_09))/sd(null_07_09)))*2
  p_07_09 <- sum(real$act_minus_deactivated[4] < null_07_09)/1000

  print(paste0("Difference in bin 01_03 is ", real$act_minus_deactivated[1], "P value is ",  p_01_03))
  print(paste0("Difference in bin 03_05 is ", real$act_minus_deactivated[2], "P value is ",  p_03_05))
  print(paste0("Difference in bin 05_07 is ", real$act_minus_deactivated[3], "P value is ",  p_05_07))
  print(paste0("Difference in bin 07_09 is ", real$act_minus_deactivated[4], "P value is ",  p_07_09))
  
}


# WM_Task_2bk_Acc
plot_pred_acc_bin("gordon_nested_permutation_test_12bin_WM_Task_2bk_Acc.csv", "2-back Task")
linear_trend_stats("gordon_nested_permutation_test_12bin_WM_Task_2bk_Acc.csv")
compare_pred_acc_bins("gordon_nested_permutation_test_12bin_WM_Task_2bk_Acc.csv")

# PicVocab_AgeAdj
plot_pred_acc_bin("gordon_nested_permutation_test_12bin_PicVocab_AgeAdj.csv", "Pic-vocab Task")
linear_trend_stats("gordon_nested_permutation_test_12bin_PicVocab_AgeAdj.csv")
compare_pred_acc_bins("gordon_nested_permutation_test_12bin_PicVocab_AgeAdj.csv")

# PMAT24_A_CR
plot_pred_acc_bin("gordon_nested_permutation_test_12bin_PMAT24_A_CR.csv", "PMAT Task")
linear_trend_stats("gordon_nested_permutation_test_12bin_PMAT24_A_CR.csv")
compare_pred_acc_bins("gordon_nested_permutation_test_12bin_PMAT24_A_CR.csv")

# ReadEng_AgeAdj
plot_pred_acc_bin("gordon_nested_permutation_test_12bin_ReadEng_AgeAdj.csv", "Reading-cog Task")
linear_trend_stats("gordon_nested_permutation_test_12bin_ReadEng_AgeAdj.csv")
compare_pred_acc_bins("gordon_nested_permutation_test_12bin_ReadEng_AgeAdj.csv")

# ListSort_AgeAdj
plot_pred_acc_bin("gordon_nested_permutation_test_12bin_ListSort_AgeAdj.csv", "List-sorting Task")
linear_trend_stats("gordon_nested_permutation_test_12bin_ListSort_AgeAdj.csv")
compare_pred_acc_bins("gordon_nested_permutation_test_12bin_ListSort_AgeAdj.csv")
```


```{r}
rm(list = ls())
df_2back <- import(here::here("data/gordon_permutation/gordon_nested_permutation_test_12bin_WM_Task_2bk_Acc.csv"))[1,] %>%
  pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
               values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
  mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Reverse-Activated"),
         condition = rep("2-back Task", nrow(.))) %>%
  select(-load_insensitive)

df_ls <- import(here::here("data/gordon_permutation/gordon_nested_permutation_test_12bin_ListSort_AgeAdj.csv"))[1,] %>%
  pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
               values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
  mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Reverse-Activated"),
         condition = rep("List-sorting Task", nrow(.))) %>%
  select(-load_insensitive)

df_vocab <- import(here::here("data/gordon_permutation/gordon_nested_permutation_test_12bin_PicVocab_AgeAdj.csv"))[1,] %>%
  pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
               values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
  mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Reverse-Activated"),
         condition = rep("Vocab Task", nrow(.))) %>%
  select(-load_insensitive)

df_pmat <- import(here::here("data/gordon_permutation/gordon_nested_permutation_test_12bin_PMAT24_A_CR.csv"))[1,] %>%
  pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
               values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
  mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Reverse-Activated"),
         condition = rep("PMAT Task", nrow(.))) %>%
  select(-load_insensitive)

df_reading <- import(here::here("data/gordon_permutation/gordon_nested_permutation_test_12bin_ReadEng_AgeAdj.csv"))[1,] %>%
  pivot_longer(., -load_insensitive, names_to = c("parcel_type","effect_size"), 
               values_to = "pred_acc", names_pattern = "(.*)_(.*_.*)" ) %>%
  mutate(parcel_type = ifelse(parcel_type == "act", "Activated", "Reverse-Activated"),
         condition = rep("Reading-cog Task", nrow(.))) %>%
  select(-load_insensitive)

rbind(df_vocab, df_pmat, df_reading, df_2back, df_ls) %>%
  mutate(condition = factor(condition, levels = c("PMAT Task", "Reading-cog Task", "Vocab Task", "2-back Task", "List-sorting Task"))) %>%
  ggplot(aes(x = effect_size, y = pred_acc, color = parcel_type)) +
  facet_wrap(vars(condition), scales = "free", nrow = 2) + 
  geom_point() + 
  geom_line(aes(group = interaction(condition,parcel_type), color = parcel_type)) + 
  ylim(0, 0.6) + 
  xlab("Parcel Bins based on ES") + 
  ylab("Predictive Accuracy (Measured by r)") + 
  scale_x_discrete(labels=c("load_insensitive" = "Insensitive", 
                            "01_03" = "(0.1,0.3)", "03_05" = "(0.3,0.5)", "05_07" = "(0.5,0.7)", 
                            "07_09" = "(0.7,0.9)", "09_11" = "(0.9,1.1)", "11_13" = "(1.1,1.3)",
                            "13_15" = "(1.3,1.5)"),
                   guide = guide_axis(n.dodge = 2)) + 
  theme_classic() +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10, face = "bold"),
        axis.text.y = element_text(size = 8, face = "bold"), 
        plot.title = element_text(size = 10, hjust = 0.5),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.position = c(0.1, 0.92),
        legend.title = element_blank(),
        strip.text.x = element_text(size = 10, face = 'bold'))  #+ 
  #ggsave("plots/suppmat/gordon/gordon_multivariate_facet.svg", width=7, height=4.5)
```

### Use load-effect to guide feature selection
```{r}
rm(list = ls())

plot_feature_selection <- function(df_dir){ # df_dir = "gordon_feature_selection_permutation_WM_Task_2bk_Acc.csv"
  # read in data
  df <- import(here::here(paste0("data/gordon_permutation/", df_dir)))
  # extract null 
  null <- df %>%
    mutate(idx = seq(1,60)) %>%
    select(lower, mean, upper, idx)
  # plot
  plot <- 
    df %>%
      ggplot(aes(x = idx, y = observed)) + 
      geom_point(aes(color = parcel_type), size = 2) + 
      geom_line(color = "black", size = 1) + 
      geom_point(data = null, aes(x = idx, y = mean), alpha = 0.1, color = "gray1") + 
      geom_line(data = null, aes(x = idx, y = mean), color = "gray1", alpha = 0.1) +
      geom_ribbon(data = null, aes(x = idx, ymin = lower, ymax = upper), fill = "gray1", 
                  linetype = 2, alpha = 0.1, inherit.aes = F) + 
      theme_minimal() + 
      ylab("Predictive Acc") + 
      xlab("Top n parcels \n (Based on CohenD)") + 
      scale_x_continuous(breaks=seq(0, 60, 10))+ 
      theme(axis.title.x = element_text(size = 13, face = 'bold'),
            axis.text.x = element_text(size = 10, face = 'bold'),
            axis.title.y = element_text(size = 13, face = "bold"),
            axis.text.y = element_text(size = 13, face = "bold"), 
            strip.text.x = element_text(size = 13, face = "bold"),
            plot.margin = margin(1,1,0,1,"cm"),
            legend.position = "none",
            legend.title = element_blank()) 
  
  return(plot)
}

# WM_Task_2bk_Acc
plot_feature_selection("gordon_feature_selection_permutation_WM_Task_2bk_Acc.csv")
# PicVocab_AgeAdj
plot_feature_selection("gordon_feature_selection_permutation_PicVocab_AgeAdj.csv")
# PMAT24_A_CR
plot_feature_selection("gordon_feature_selection_permutation_PMAT24_A_CR.csv")
# ReadEng_AgeAdj
plot_feature_selection("gordon_feature_selection_permutation_ReadEng_AgeAdj.csv")
# ListSort_AgeAdj
plot_feature_selection("gordon_feature_selection_permutation_ListSort_AgeAdj.csv")

```


```{r}
rm(list = ls())

# ----------
# Figure 7a
# ----------

# 2back
df_wm <- import(here::here('data/gordon_permutation/gordon_feature_selection_permutation_WM_Task_2bk_Acc.csv')) %>%
  mutate(rep("2-back", nrow(.)))
observe_wm <- df_wm %>%
  select(observed, idx, parcel_type) %>%
  rename("2-back_predAcc" = observed)
null_wm <- df_wm %>%
    select(lower, mean, upper, idx) %>%
    rename("2-back_predAcc_lower" = lower, 
           "2-back_predAcc_mean" = mean,
           "2-back_predAcc_upper" = upper)

# list-sorting
df_ls <- import(here::here('data/gordon_permutation/gordon_feature_selection_permutation_ListSort_AgeAdj.csv')) %>%
   mutate(rep("List-sorting", nrow(.)))
observe_ls <- df_ls %>%
  select(observed, idx, parcel_type) %>%
  rename("List-sorting_predAcc" = observed)
null_ls <- df_ls %>%
    select(lower, mean, upper, idx) %>%
    rename("List-sorting_predAcc_lower" = lower, 
           "List-sorting_predAcc_mean" = mean,
           "List-sorting_predAcc_upper" = upper)

# pmat
df_pmat <- import(here::here('data/gordon_permutation/gordon_feature_selection_permutation_PMAT24_A_CR.csv')) %>%
  mutate(rep("PMAT", nrow(.)))
observe_pmat <- df_pmat %>%
  select(observed, idx, parcel_type) %>%
  rename("PMAT_predAcc" = observed)
null_pmat <- df_pmat %>%
    select(lower, mean, upper, idx) %>%
    rename("PMAT_predAcc_lower" = lower, 
           "PMAT_predAcc_mean" = mean,
           "PMAT_predAcc_upper" = upper)

# reading-cog
df_reading <- import(here::here('data/gordon_permutation/gordon_feature_selection_permutation_ReadEng_AgeAdj.csv')) %>%
   mutate(rep("Reading-cog", nrow(.)))
observe_reading <- df_reading %>%
  select(observed, idx, parcel_type) %>%
  rename("Reading-cog_predAcc" = observed)
null_reading <- df_reading %>%
    select(lower, mean, upper, idx) %>%
    rename("Reading-cog_predAcc_lower" = lower, 
           "Reading-cog_predAcc_mean" = mean,
           "Reading-cog_predAcc_upper" = upper)

# vocab
df_vocab <- import(here::here('data/gordon_permutation/gordon_feature_selection_permutation_PicVocab_AgeAdj.csv')) %>%
   mutate(rep("Vocab", nrow(.)))
observe_vocab <- df_vocab %>%
  select(observed, idx, parcel_type) %>%
  rename("Vocab_predAcc" = observed)
null_vocab <- df_vocab %>%
    select(lower, mean, upper, idx) %>%
    rename("Vocab_predAcc_lower" = lower, 
           "Vocab_predAcc_mean" = mean,
           "Vocab_predAcc_upper" = upper)

null_cut <- left_join(null_pmat, null_reading) %>% left_join(.,  null_vocab) %>% left_join(.,  null_wm) %>% left_join(.,  null_ls) %>%
  pivot_longer(., cols = -idx, names_pattern = "(.*)_predAcc_(.*)", names_to = c("beh_label","value_type"), values_to = "value") %>%
  pivot_wider(., names_from = "value_type", values_from = "value" ) %>% 
  mutate(beh_label = factor(beh_label, levels = c("PMAT", "Reading-cog", "Vocab", "2-back", "List-sorting"))) 
  

left_join(observe_pmat, observe_reading,  by = c("idx","parcel_type")) %>% left_join(., observe_vocab, by = c("idx","parcel_type")) %>% left_join(., observe_wm, by = c("idx","parcel_type")) %>% left_join(., observe_ls, by = c("idx","parcel_type")) %>%
  pivot_longer(., cols = c("PMAT_predAcc", "Reading-cog_predAcc", "Vocab_predAcc", "2-back_predAcc", "List-sorting_predAcc"), names_pattern = "(.*)_(.*)", names_to = c("beh_label","value_type"), values_to = "value") %>% 
  mutate(beh_label = factor(beh_label, levels = c("PMAT", "Reading-cog", "Vocab", "2-back", "List-sorting"))) %>%
  ggplot(aes(x = idx, y = value)) + 
  facet_wrap(vars(beh_label), scales = "free", nrow = 2) + 
  geom_point(aes(color = parcel_type), size = 0.6) + 
  geom_line(color = "black", size = 0.5) + 
  geom_point(data = null_cut, aes(x = idx, y = mean), alpha = 0.2, color = "grey", size = 0.6) + 
  geom_line(data = null_cut, aes(x = idx, y = mean), color = "grey", size = 0.5) +
  geom_ribbon(data = null_cut, aes(x = idx, ymin = lower, ymax = upper), fill = "grey", 
              linetype = 2, alpha = 0.2, inherit.aes = F) + 
  theme_classic() + 
  ylab("Predictive Acc") + 
  xlab("Top n parcels") + 
  scale_x_continuous(breaks=seq(0, 60, 10))+ 
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(size = 8),
        axis.title.y = element_text(size = 10),
        axis.text.y = element_text(size = 8), 
        plot.title = element_text(size = 10),
        strip.text.x = element_text(size = 10),
        plot.margin = margin(0,0,0,0,"cm"),
        legend.position = "none",
        legend.title = element_blank()) #+
  #ggsave("plots/suppmat/gordon/model_building.svg",  width=7, height=4.5)
```

