{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "import random, sys, os, multiprocessing\n",
    "from itertools import chain\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr, sem\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing cohen's D \n",
    "def _cohenD(df, parcel_name):\n",
    "    dat = list(df[parcel_name])\n",
    "    d = np.mean(dat)/np.std(dat, ddof = 1)  \n",
    "    return d\n",
    "\n",
    "# svr pipeline for each sampling bin \n",
    "def _svr_per_bin(merged_df, lab_type, parcel_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Main function that use SVR to measure a list of parcel's predictive accuracy for the selected beh performance\n",
    "    \n",
    "    Parameters:\n",
    "    merged_df: a pandas dataframe that include neural and behaviral data. Column names include parcel name and beh measure names\n",
    "    parcel_list: a list of parcel names (i.e., string), that will match the column names in merged_df\n",
    "    lab_type: a string indicating the type of behaviral measure could be any of the following 5 strings: \n",
    "              1) WM_Task_2bk_Acc 2) ListSort_AgeAdj 3) PMAT24_A_CR 4) PicVocab_AgeAdj 5) ReadEng_AgeAdj\n",
    "    \n",
    "    Retrun:\n",
    "    cur_bin_pred_acc: A single number that is the averaged of the 10 fold CV, measuring the predictive acc of the input parcel list. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle = False) # 10 fold\n",
    "    feature = merged_df[parcel_list].to_numpy() # features of the current parcel_list\n",
    "    label = merged_df[lab_type].to_numpy() # beh score\n",
    "\n",
    "    pred_acc_list = [] \n",
    "    for train_index, test_index in kf.split(merge_df):\n",
    "\n",
    "        train_feature, train_label = feature[train_index], label[train_index] # training feature and label\n",
    "        test_feature, test_label = feature[test_index], label[test_index] # testing feature and label\n",
    "\n",
    "        svr_pipeline = make_pipeline(StandardScaler(), SVR(kernel='linear',degree=1,\n",
    "                                                       C=1.0,epsilon=0.1)) # svr pipeline\n",
    "        svr_pipeline.fit(train_feature, train_label)  # train the model \n",
    "        pred_test_label = svr_pipeline.predict(test_feature) # test the model \n",
    "        pred_acc = pearsonr(test_label, pred_test_label)[0] # check pred acc \n",
    "        pred_acc_list.append(pred_acc) \n",
    "    \n",
    "    cur_bin_pred_acc = np.mean(pred_acc_list) # mean of 10 folds\n",
    "    return(cur_bin_pred_acc)\n",
    "\n",
    "# randomly sample 10 parcels from the given bin list for N iterations. Getting N sampling data\n",
    "def _sample_from_bin(parcel_bin, sample_size, iteration):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to do random sampling from each bin for N iteration \n",
    "    \n",
    "    Parameters:\n",
    "    bin_list: a list  containing the parcel names (i.e., string) of parcels in this bin\n",
    "    iteration: int, how many samples to draw\n",
    "    \n",
    "    Retrun:\n",
    "    iteration_sampling:  A list of all samplings. Each sampling is a list 10 parcels. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seed_list = random.sample(range(1, 10000), iteration) # get seed\n",
    "    if len(set(seed_list)) != iteration: # make sure seeds are unique\n",
    "        print(\"seed duplication\")  \n",
    "    else:\n",
    "        iteration_sampling = []\n",
    "        for seed in seed_list: # for each given seed\n",
    "            random.seed(seed)\n",
    "            cur_iteration = random.sample(parcel_bin,sample_size)\n",
    "            iteration_sampling.append(cur_iteration)\n",
    "    \n",
    "    return(iteration_sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictive accuracy using full set features for each task (Gordon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_data = pd.read_csv('/home/peetal/hulacon/nested_permutation/gordon333_cope11_rm_outlier.csv')\n",
    "\n",
    "full_beh = pd.read_csv('/home/peetal/hulacon/nested_permutation/HCP_behavioral_data.csv') # behavioral\n",
    "beh = full_beh[['Subject','WM_Task_2bk_Acc', 'ListSort_AgeAdj', 'PMAT24_A_CR', 'PicVocab_AgeAdj', 'ReadEng_AgeAdj']] # ID, in-scanner, and out-scanner task\n",
    "\n",
    "# Effect size (Cohen's D) information\n",
    "parcel_es = pd.DataFrame(columns = ['parcel_name','es']) # new df for cohen's D per parcel\n",
    "parcel_es['parcel_name'] = list(neural_data.columns[1:])\n",
    "parcel_es['es'] = [_cohenD(neural_data, parcel) for parcel in list(neural_data.columns[1:])]\n",
    "parcel_es['abs_es'] = abs(parcel_es['es'])\n",
    "parcel_es = parcel_es.sort_values(by=['abs_es'], ascending = False)\n",
    "\n",
    "# join neural and behaviral data by subject id\n",
    "merge_df = pd.merge(neural_data, beh, how='left', on=['Subject']).dropna() # join with ID\n",
    "\n",
    "full_feature_acc = []\n",
    "pool = multiprocessing.Pool(processes = 16)\n",
    "\n",
    "for beh_lab in ['WM_Task_2bk_Acc', 'ListSort_AgeAdj', 'PMAT24_A_CR', 'PicVocab_AgeAdj', 'ReadEng_AgeAdj']:\n",
    "    svr_per_shuffle_partial = partial(_svr_per_bin, merge_df, beh_lab)\n",
    "\n",
    "    all_parcel_bin_list = list(parcel_es['parcel_name'])\n",
    "    full_feature_acc.append(pool.map(svr_per_shuffle_partial, [all_parcel_bin_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.47225339973112324],\n",
       " [0.1901933952318764],\n",
       " [0.28288049279231486],\n",
       " [0.2598319542752106],\n",
       " [0.26017998357567096]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_feature_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictive accuracy using full set features for each task (Schaefer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_data = pd.read_csv('/home/peetal/hulacon/nested_permutation/schaefer400_cope11_rm_outlier.csv') # neural\n",
    "full_beh = pd.read_csv('/home/peetal/hulacon/nested_permutation/HCP_behavioral_data.csv') # behavioral\n",
    "beh = full_beh[['Subject','WM_Task_2bk_Acc', 'ListSort_AgeAdj',\n",
    "                'PMAT24_A_CR', 'PicVocab_AgeAdj', 'ReadEng_AgeAdj']] # ID, in-scanner, and out-scanner task\n",
    "\n",
    "# Effect size (Cohen's D) information\n",
    "parcel_es = pd.DataFrame(columns = ['parcel_name','es']) # new df for cohen's D per parcel\n",
    "parcel_es['parcel_name'] = list(neural_data.columns[1:])\n",
    "parcel_es['es'] = [_cohenD(neural_data, parcel) for parcel in list(neural_data.columns[1:])]\n",
    "parcel_es['abs_es'] = abs(parcel_es['es'])\n",
    "parcel_es = parcel_es.sort_values(by=['abs_es'], ascending = False)\n",
    "\n",
    "# join neural and behaviral data by subject id\n",
    "merge_df = pd.merge(neural_data, beh, how='left', on=['Subject']).dropna() # join with ID\n",
    "\n",
    "full_feature_acc = []\n",
    "pool = multiprocessing.Pool(processes = 16)\n",
    "\n",
    "for beh_lab in ['WM_Task_2bk_Acc', 'ListSort_AgeAdj', 'PMAT24_A_CR', 'PicVocab_AgeAdj', 'ReadEng_AgeAdj']:\n",
    "    svr_per_shuffle_partial = partial(_svr_per_bin, merge_df, beh_lab)\n",
    "\n",
    "    all_parcel_bin_list = list(parcel_es['parcel_name'])\n",
    "    full_feature_acc.append(pool.map(svr_per_shuffle_partial, [all_parcel_bin_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.451644173072573],\n",
       " [0.1776145396326645],\n",
       " [0.3068730618944142],\n",
       " [0.1994898404213258],\n",
       " [0.2641816550473497]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_feature_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def rewrite_df(df_dir):\n",
    "    \n",
    "#    full_dir = '/projects/hulacon/peetal/nested_permutation/result/' + df_dir\n",
    "#    df = pd.read_csv(full_dir)\n",
    "#    df['current_parcel'] = list(parcel_es['parcel_name'][0:60])\n",
    "#    df['parcel_type'] = np.where(parcel_es['es'][0:60] > 0, \"activated\", \"deactivated\")\n",
    "#    df['idx'] = list(range(1,61))\n",
    "\n",
    "#    df.to_csv(full_dir, index = False)\n",
    "    \n",
    "#rewrite_df(\"schaefer_feature_selection_permutation_WM_Task_2bk_Acc.csv\")\n",
    "#rewrite_df(\"schaefer_feature_selection_permutation_ListSort_AgeAdj.csv\")\n",
    "#rewrite_df(\"schaefer_feature_selection_permutation_PicVocab_AgeAdj.csv\")\n",
    "#rewrite_df(\"schaefer_feature_selection_permutation_PMAT24_A_CR.csv\")\n",
    "#rewrite_df(\"schaefer_feature_selection_permutation_ReadEng_AgeAdj.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
